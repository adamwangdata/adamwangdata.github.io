{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfd749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033a12a",
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "title": "Macros Setup"
   },
   "source": [
    "$$\n",
    "\\newcommand{\\parens}[1]{\\mathopen{}\\left(#1\\right)\\mathclose{}}\n",
    "\\newcommand{\\bracks}[1]{\\mathopen{}\\left[#1\\right]\\mathclose{}}\n",
    "\\newcommand{\\braces}[1]{\\mathopen{}\\left\\{#1\\right\\}\\mathclose{}}\n",
    "\\newcommand{\\abs}[1]{\\mathopen{}\\left\\lvert#1\\right\\rvert\\mathclose{}}\n",
    "\\newcommand{\\norm}[1]{\\mathopen{}\\left\\lVert#1\\right\\rVert\\mathclose{}}\n",
    "\\renewcommand{\\vec}[1]{\\boldsymbol{\\mathbf{#1}}}\n",
    "\\newcommand{\\mat}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\tpose}[1]{#1^T}\n",
    "\\newcommand{\\inv}[1]{#1^{-1}}\n",
    "\\newcommand{\\Matrix}[1]{\n",
    "  \\begin{bmatrix}\n",
    "    #1\n",
    "  \\end{bmatrix}\n",
    "}\n",
    "\\newcommand{\\seq}[1]{1, 2, \\ldots, #1}\n",
    "\\newcommand{\\reals}{\\mathbb{R}}\n",
    "\\newcommand{\\mper}{\\,\\text{.}}\n",
    "\\newcommand{\\mcom}{\\,\\text{,}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc87ff5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "(yelp)=\n",
    "# Farmer's Fridge Yelp Reviews Analysis\n",
    "\n",
    "## Summary\n",
    "\n",
    "Using data from 173 Yelp reviews for three major fridge locations, I examined trends like average ratings over time, item popularity, and common phrases mentioned in reviews.\n",
    "The results show that, pre-pandemic:\n",
    "\n",
    "1. The airport location had more and, on average, better ratings while it was active\n",
    "2. Salad popularity decreased over time while bowl and sandwich popularities increased, but salads are still by far the most popular type of item\n",
    "\n",
    "Common positive phrases include \"healthy\", \"good\", and \"fresh\", while analysis of negative reviews suggests price and portion sizes factored into their low ratings.\n",
    "\n",
    "```{note}\n",
    "You can run and modify the code on this page Jupyter Notebook style, but without leaving the page!\n",
    "Hover over the {fa}`rocket` launch button at the top of the page, then click the {guilabel}`Live Code` button.\n",
    "Once you see \"Launching from mybinder.org: ready\", you can run code cells.\n",
    "Refresh the page to revert to the original view.\n",
    "```\n",
    "\n",
    "## Data Source\n",
    "\n",
    "I scraped Yelp reviews using the Selenium package on 6/20/2021.\n",
    "Data include rating (1 out of 5 stars), the main text of the review, and metadata like the the number of Yelp reviews a user has given or the number of people who found a review useful.\n",
    "Each review constitutes a row in the `yelp.csv` file, with each piece of data (rating, text, etc.) as a separate column.\n",
    "\n",
    "I scraped three different Chicago fridges.\n",
    "I found the locations by manually navigating Yelp's website and searching for \"farmer's fridge\" in Chicago and selecting fridges with > 10 reviews.\n",
    "The three fridges have [93 reviews](https://www.yelp.com/biz/farmers-fridge-chicago-13), [23 reviews](https://www.yelp.com/biz/farmers-fridge-chicago-14), and [57 reviews](https://www.yelp.com/biz/farmers-fridge-chicago-87) for a total of 173 reviews.\n",
    "Two fridges are located in The Loop, while the third location with 57 reviews is located in O'Hare International Airport.\n",
    "\n",
    "My complete code to scrape the reviews can be toggled below, but is not the main focus of this writeup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55fbdd",
   "metadata": {},
   "source": [
    "\n",
    "````{toggle}\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def scrape_reviews(yelp_url, sleep_time=1):\n",
    "    \"\"\"Retrieve all text data from reviews, including star rating.\n",
    "    Sleep between pages of reviews so the html loads before accessing data.\"\"\"\n",
    "    browser = webdriver.Firefox()\n",
    "    browser.get(yelp_url)\n",
    "\n",
    "    all_reviews = []\n",
    "    more_pages_exist = True\n",
    "    while more_pages_exist:\n",
    "        time.sleep(sleep_time)  # Wait for page to load\n",
    "        reviews = get_reviews_on_page(browser)\n",
    "        all_reviews.extend(reviews)\n",
    "        click_succeeded = click_on_next_page(browser)\n",
    "        if not click_succeeded:\n",
    "            more_pages_exist = False\n",
    "    return all_reviews\n",
    "\n",
    "def get_reviews_on_page(browser):\n",
    "    \"\"\"Extract reviews on a Yelp page.\"\"\"\n",
    "    reviews = []\n",
    "    review_num = 1\n",
    "    more_reviews_exist = True\n",
    "    while more_reviews_exist:\n",
    "        try:\n",
    "            review = browser.find_element_by_xpath(\n",
    "                \"/html/body/div[2]/div[2]/yelp-react-root/div/div[3]/div/div\"\n",
    "                + \"/div[2]/div/div[1]/div[2]/section[2]/div[2]/div/ul\"\n",
    "                + \"/li[\" + str(review_num) + \"]\"\n",
    "                + \"/div\"\n",
    "            )\n",
    "            rating = browser.find_element_by_xpath(\n",
    "                \"/html/body/div[2]/div[2]/yelp-react-root/div/div[3]/div/div/\"\n",
    "                + \"div[2]/div/div[1]/div[2]/section[2]/div[2]/div/ul\"\n",
    "                + \"/li[\" + str(review_num) + \"]\"\n",
    "                + \"/div/div[2]/div/div[1]/span/div\"\n",
    "            ).get_attribute('aria-label')\n",
    "\n",
    "            reviews.append(rating + \"\\n\" + review.text)\n",
    "            review_num += 1\n",
    "        except NoSuchElementException:  # No more reviews on page\n",
    "            more_reviews_exist = False\n",
    "    return reviews\n",
    "\n",
    "def click_on_next_page(browser):\n",
    "    \"\"\"Click on the next page of Yelp reviews, if it exists. Returns\n",
    "    whether the next page exists.\"\"\"\n",
    "    try:\n",
    "        next_page_button = browser.find_element_by_css_selector(\n",
    "            '.next-link'\n",
    "        )\n",
    "        next_page_button.click()\n",
    "        return True\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "\n",
    "def write_to_csv(file_name, reviews, append=False, location=''):\n",
    "    \"\"\"Write the following data as columns in a csv file:\n",
    "        - rating (1 to 5 scale)\n",
    "        - reviewer data: name, location, number of friends, number of\n",
    "            reviews, and number of photos\n",
    "        - review data: date of review, updated or not, review text,\n",
    "            number of people who found the review useful, funny, and cool\n",
    "    \"\"\"\n",
    "    mode = 'a' if append else 'w'\n",
    "    with open(file_name, mode, newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        if not append:\n",
    "            header = [\n",
    "                'rating',\n",
    "                'name',\n",
    "                'elite',\n",
    "                'reviewer_location',\n",
    "                'n_friends',\n",
    "                'n_reviews',\n",
    "                'n_photos',\n",
    "                'review_photos',\n",
    "                'date',\n",
    "                'updated',\n",
    "                'text',\n",
    "                'n_useful',\n",
    "                'n_funny',\n",
    "                'n_cool'\n",
    "            ]\n",
    "            if location:\n",
    "                header.append('restaurant_location')\n",
    "            writer.writerow(header)\n",
    "\n",
    "        for review in reviews:\n",
    "            row = process_review(review)\n",
    "            if location:\n",
    "                row.append(location)\n",
    "            writer.writerow(row)\n",
    "\n",
    "def process_review(review):\n",
    "    \"\"\"Process the structure of a Yelp review line by line into a row\n",
    "    to be appended by the `write_to_csv()` function.\"\"\"\n",
    "    lines = review.split('\\n')\n",
    "    rating = lines.pop(0)[0]\n",
    "\n",
    "    # Reviewer data\n",
    "    name = lines.pop(0)\n",
    "    if is_elite(lines[0]):\n",
    "        elite = 'True'\n",
    "        lines.pop(0)\n",
    "    else:\n",
    "        elite = 'False'\n",
    "    location = lines.pop(0)\n",
    "    n_friends = lines.pop(0)\n",
    "    n_reviews = lines.pop(0)\n",
    "    if is_int(lines[0]):\n",
    "        n_photos = lines.pop(0)\n",
    "    else:\n",
    "        n_photos = 0\n",
    "\n",
    "    # Review data\n",
    "    date = lines.pop(0)\n",
    "    if is_updated_review(lines[0]):\n",
    "        updated = 'True'\n",
    "        lines.pop(0)\n",
    "    else:\n",
    "        updated = 'False'\n",
    "    if has_photos(lines[0], single=True):\n",
    "        review_photos = '1'\n",
    "        lines.pop(0)\n",
    "    elif has_photos(lines[0], single=False):\n",
    "        review_photos = strip_nonint(lines[0], 'photos')\n",
    "        lines.pop(0)\n",
    "    else:\n",
    "        review_photos = 0\n",
    "    end_of_review_text = find_end_of_review_text(lines)\n",
    "    text = '\\n'.join(lines[:end_of_review_text])\n",
    "    n_useful = strip_nonint(lines[end_of_review_text], 'Useful')\n",
    "    n_funny = strip_nonint(lines[end_of_review_text + 1], 'Funny')\n",
    "    n_cool = strip_nonint(lines[end_of_review_text + 2], 'Cool')\n",
    "\n",
    "    row = [\n",
    "        rating,\n",
    "        name,\n",
    "        elite,\n",
    "        location,\n",
    "        n_friends,\n",
    "        n_reviews,\n",
    "        n_photos,\n",
    "        review_photos,\n",
    "        date,\n",
    "        updated,\n",
    "        text,\n",
    "        n_useful,\n",
    "        n_funny,\n",
    "        n_cool\n",
    "    ]\n",
    "    return row\n",
    "\n",
    "def find_end_of_review_text(lines):\n",
    "    \"\"\"Find which line the main text of a review ends.\"\"\"\n",
    "    for i in range(len(lines) - 2):\n",
    "        line1 = lines[i]\n",
    "        line2 = lines[i + 1]\n",
    "        line3 = lines[i + 2]\n",
    "        if (line1.startswith('Useful')\n",
    "                and line2.startswith('Funny')\n",
    "                and line3.startswith('Cool')):\n",
    "            return i\n",
    "\n",
    "    print(\"Warning: no expected end of text found.\")\n",
    "\n",
    "def is_elite(string):\n",
    "    \"\"\"Determine if given string is an elite Yelp user.\"\"\"\n",
    "    return string.startswith(\"Elite\")\n",
    "\n",
    "def is_int(string):\n",
    "    \"\"\"Determine if given string is an integer.\"\"\"\n",
    "    try:\n",
    "        int(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def has_photos(string, single=False):\n",
    "    \"\"\"Determine if given string ends in 'photo' or 'photos'.\"\"\"\n",
    "    if single:\n",
    "        return string.endswith('photo')\n",
    "    else:\n",
    "        return string.endswith('photos')\n",
    "\n",
    "def is_updated_review(string):\n",
    "    \"\"\"Determine if given string indicates an updated Yelp review.\"\"\"\n",
    "    return string == \"Updated review\"\n",
    "\n",
    "def strip_nonint(string, word_to_strip):\n",
    "    \"\"\"Strip words and spaces from a string, keeping non-integer parts\n",
    "    after stripping. If the remaining piece is not an integer, return '0'.\n",
    "    So, 'Useful 12' becomes '12' while 'Useful' becomes '0'.\"\"\"\n",
    "    stripped = string.strip(word_to_strip).strip(' ')\n",
    "    if is_int(stripped):\n",
    "        return stripped\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url_location_pairs = [\n",
    "        ('https://www.yelp.com/biz/farmers-fridge-chicago-13', 'The Loop 1'),\n",
    "        ('https://www.yelp.com/biz/farmers-fridge-chicago-14', 'The Loop 2'),\n",
    "        ('https://www.yelp.com/biz/farmers-fridge-chicago-87', \"O'Hare Airport\"),\n",
    "    ]\n",
    "    file_name = 'yelp.csv'\n",
    "\n",
    "    counter = 1  # Determine when to write to a new file or append to existing one\n",
    "    for url, location in url_location_pairs:\n",
    "        print(f\"Scraping url number {counter}\")\n",
    "        reviews = scrape_reviews(url)\n",
    "        if counter == 1:\n",
    "            write_to_csv(file_name, reviews, append=False, location=location)\n",
    "        elif counter > 1:\n",
    "            write_to_csv(file_name, reviews, append=True, location=location)\n",
    "        counter += 1\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52b570",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Data Cleaning and Exploration\n",
    "\n",
    "Let’s load in the data and examine it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebee9da1",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x85 in position 6493: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24568/309737772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yelp.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/website/env/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/website/env/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/website/env/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/website/env/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/website/env/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/website/env/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/website/env/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/projects/website/env/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/projects/website/env/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/projects/website/env/lib/python3.7/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x85 in position 6493: invalid start byte"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('yelp.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d10e63",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "The important variables that will be used for this analysis are:\n",
    "\n",
    "1. `rating`: an integer rating between 1 (worst) and 5 (best)\n",
    "2. `date`: the date the review was posted on Yelp\n",
    "3. `text`: the main contents of the review\n",
    "4. `business_location`: the location of the fridge\n",
    "\n",
    "The others are not used, but some ideas how of how to use them are sketched in § [](yelp-future).\n",
    "\n",
    "It's much more convenient to work with datetime objects, so let's convert them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947713c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492cc464",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now we can easily group reviews by year.\n",
    "Also grouping by fridge (i.e. business) location, we can count the number of reviews posted at each location in a given year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouped_by_year_and_loc = df.groupby([df.date.dt.year, df.business_location])\n",
    "annual_counts = grouped_by_year_and_loc.rating.agg('count').unstack()\n",
    "annual_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dfd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = annual_counts.plot()\n",
    "ax.set(xlabel='Year', ylabel='Number of Annual Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d309a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "The first Loop location seems to have been one of the first fridges, with reviews available in the same year the company was founded.\n",
    "The number of reviews per year begins to decline around 2016, coinciding with the opening up of the second Loop location.\n",
    "In 2017, a fridge location in O'Hare international airport opened up and became relatively popular.\n",
    "In 2020, when the effects of the COVID-19 pandemic drastically altered lives of Chicago residents, the number of reviews also drastically declined, continuing into 2021.\n",
    "\n",
    "If we're interested in annual trends over time, it may helpful to combine the two Loop locations to avoid aggregates from only 5 or fewer reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41739f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def combine_loop_locations(locations):\n",
    "    combined_locations = []\n",
    "    for loc in locations:\n",
    "        if loc.startswith(\"The Loop\"):\n",
    "            combined_locations.append(\"The Loop\")\n",
    "        else:\n",
    "            combined_locations.append(loc)\n",
    "    return combined_locations\n",
    "\n",
    "df['combined_location'] = combine_loop_locations(df.business_location)\n",
    "np.unique(df.combined_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3de31c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "This section examines a few specific questions, focusing on graphical presentation and inuition rather than formal statistical tests.\n",
    "These analyses can help inform what avenues Farmer's Fridge could look into in the future and serve as a foundation for more detailed analyses later.\n",
    "\n",
    "### Ratings Over Time\n",
    "\n",
    "A plot of annual ratings over time, grouped by location, can be constructed by grouping by the relevant variables and invoking the `.plot()` method.\n",
    "In addition, I'll overlay points with sizes proportional to the number of reviews.\n",
    "This allows us to quickly weight large points more than small points.\n",
    "This is important especially for 2020 and 2021 when there is very little data;\n",
    "for example, the 5/5 rating data for the Loop locations in 2021 is only based on 3 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0cbe4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "grouped_by_year_and_loc = df.groupby([df.date.dt.year, df.combined_location])\n",
    "annual_ratings = grouped_by_year_and_loc.rating.agg('mean').unstack()\n",
    "annual_counts = grouped_by_year_and_loc.rating.agg('count').unstack()\n",
    "\n",
    "ax = annual_ratings.plot()\n",
    "ax.set(xlabel='Year', ylabel='Average Annual Rating (1 to 5)')\n",
    "for i, loc in enumerate(annual_ratings.columns):\n",
    "    col = 'C' + str(i)\n",
    "    point_sizes = [5*count for count in annual_counts[loc]]\n",
    "    ax.scatter(annual_ratings.index, annual_ratings[loc],\n",
    "               s=point_sizes, c=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff8cd1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "There appears to have been a steady decline in rating throughout 2016.\n",
    "In 2017, the Loop location's rating fell substantially but the introduction of the O'Hare location was a success.\n",
    "The Loop location rebounded somewhat and the O'Hare location continued to see success, until the pandemic hit, drastically lowering the number of ratings available in 2020 and 2021.\n",
    "\n",
    "### What Items are Popular?\n",
    "\n",
    "A simple analysis assumes a correlation between the number of mentions of an item and its popularity.\n",
    "I've grouped the items like Farmer's Fridge's [website menu](https://www.farmersfridge.com/menu), as well as words that are likely to belong in that grouping. Terms synonymous with salad, for example, are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb579b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "salads = [\n",
    "    'Smoked Cheddar Cobb Salad',\n",
    "    'Buffalo Chicken Ranch Salad',\n",
    "    'Elote Salad',\n",
    "    'North Napa Salad',\n",
    "    'Grilled Chicken Caesar Salad',\n",
    "    'Southwest Salad with Chipotle Chicken',\n",
    "    'Green Goddess Salad',\n",
    "    'Greek Salad',\n",
    "    'Salad',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23541f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "The other groupings are defind as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1316070",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "bowls = [\n",
    "    'Apple Cinnamon Oats',\n",
    "    'Falafel Bowl',\n",
    "    'Pesto Pasta Bowl',\n",
    "    'Thai Noodle Bowl',\n",
    "    'Burrito Bowl',\n",
    "    'Red Chile Braised Pork Bowl by Rick Bayless',\n",
    "    'Pineapple Coconut Chia Pudding',\n",
    "    'Berries & Granola Greek Yogurt',\n",
    "    'Almond Butter Oats Bowl',\n",
    "    'Chickpea Tikka Masala',\n",
    "    'Chicken Tikka Masala',\n",
    "    'Grilled Chicken & Veggie Bowl',\n",
    "    'Truffle Couscous with Chicken'\n",
    "    'Bowl',\n",
    "    'oats',\n",
    "    'yogurt'\n",
    "]\n",
    "sandwiches = [\n",
    "    'Italian Turkey Wrap',\n",
    "    'Turkey, Apple & White Cheddar Wrap',\n",
    "    'Sandwich',\n",
    "    'Wrap'\n",
    "]\n",
    "snacks = [\n",
    "    'White Cheddar Cheese',\n",
    "    'Cookie Dough Bites',\n",
    "    'Dark Chocolate Trail Mix',\n",
    "    'Chips & Guac',\n",
    "    'snack',\n",
    "    'trail',\n",
    "    'dough'\n",
    "]\n",
    "proteins = [\n",
    "    'Falafel',\n",
    "    'Grilled Chicken',\n",
    "]\n",
    "drinks = [\n",
    "    'La Colombe Vanilla Draft Latte',\n",
    "    'La Colombe Triple Draft Latte',\n",
    "    'Brew Dr. Kombucha Love',\n",
    "    'Spindrift Raspberry Lime',\n",
    "    'drink'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc35c3",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "With the item groups defined, let's count unique mentions of each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_item_mentions(reviews, item_group):\n",
    "    \"\"\"Compute percentage of times an item in `item_group` is mentioned\n",
    "    in a review.\"\"\"\n",
    "    count = 0\n",
    "    for text in reviews:\n",
    "        text = text.lower()\n",
    "        for item in item_group:\n",
    "            item = item.lower()\n",
    "            if item in text:\n",
    "                count += 1\n",
    "                break\n",
    "    return count / len(reviews)\n",
    "\n",
    "item_mention_counts = {\n",
    "    'salads': count_item_mentions(df.text, salads),\n",
    "    'bowls': count_item_mentions(df.text, bowls),\n",
    "    'sandwiches': count_item_mentions(df.text, sandwiches),\n",
    "    'snacks': count_item_mentions(df.text, snacks),\n",
    "    'proteins': count_item_mentions(df.text, proteins),\n",
    "    'drinks': count_item_mentions(df.text, drinks)\n",
    "}\n",
    "item_mention_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0195e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(list(item_mention_counts.keys()), list(item_mention_counts.values()))\n",
    "ax.set(xlabel='Fraction of times mentioned in a review')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95499898",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "Salads are the overwhelming favorite.\n",
    "Have these trends changed over time?\n",
    "Below is the annual share for each item type, i.e. the fraction of times an item was mentioned in a review for a given year.\n",
    "Weighting by total number of reviews that year is again used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6522398",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def annual_item_share_plot(ax, item_group, title):\n",
    "    annual_share = grouped_by_year_and_loc.text.apply(\n",
    "        count_item_mentions, item_group=item_group\n",
    "    ).unstack()\n",
    "    annual_share.plot(ax=ax)\n",
    "    ax.set(xlabel='Year', ylabel='Annual share', title=title)\n",
    "    for i, loc in enumerate(annual_counts.columns):\n",
    "        col = 'C' + str(i)\n",
    "        point_sizes = [5*count for count in annual_counts[loc]]\n",
    "        ax.scatter(annual_share.index, annual_share[loc],\n",
    "                   s=point_sizes, c=col)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(9.6, 7.2),\n",
    "                        sharex=True, sharey=True)\n",
    "annual_item_share_plot(axs[0][0], salads, 'Salad Popularity')\n",
    "annual_item_share_plot(axs[0][1], bowls, 'Bowl Popularity')\n",
    "annual_item_share_plot(axs[1][0], sandwiches, 'Sandwich Popularity')\n",
    "annual_item_share_plot(axs[1][1], snacks, 'Snack Popularity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6580fd77",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "Salad popularity has been decreasing, but still has the largest share.\n",
    "Bowl popularity seems to have the opposite trend, though still have a smaller share.\n",
    "Sandwiches seem popular at airports, with a sudden drop in 2020 (perhaps due to the pandemic, as sandwiches are not usually eaten with utensils?)\n",
    "Finally, snacks were initially popular (perhaps because of vending machine familiarity?) but have somewhat declined, except for 2021 where there are only 3 data points.\n",
    "\n",
    "### What Phrases are Common?\n",
    "\n",
    "We can answer this question with more sophisticated language processing.\n",
    "Using NLKT, the Natural Language Toolkit, we can parse out individual words in a string of unstructured text, or more generally ngrams that contain n words strung together.\n",
    "Before any of that, let's define a function that combines all the reviews in the `df.text` column of a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3531e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_review_text(df):\n",
    "    review_text = \"\"\n",
    "    for text in df.text:\n",
    "        review_text += text + \"\\n\"*3\n",
    "    return review_text\n",
    "\n",
    "text = combine_review_text(df)\n",
    "print(text[:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0b6d3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Next, let's import `nltk` and get a list of words.\n",
    "Technically in language processing terms, we are extracting *tokens*, since objects like punctuation are retained.\n",
    "It's also useful to drop the most common words like \"to\", \"then\", and \"the\" using `nltk`'s library of English stop words and convert everything to lowercase for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = nltk.word_tokenize(text)\n",
    "filtered_lowercase_words = []\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    if word not in stop_words:\n",
    "        filtered_lowercase_words.append(word)\n",
    "filtered_lowercase_words[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237f479",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "The result is terse, but the meaning of sentences are maintained.\n",
    "\n",
    "Now we're ready to count the most common words and display them.\n",
    "I'll define functions for general ngrams for later use.\n",
    "So even though it is overkill to use a \"onegram\" for just a list of words we already have, I will for generalizibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908cbff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def count_ngrams(ngrams):\n",
    "    \"\"\"Return a dictionary of ngram, count pairs.\"\"\"\n",
    "    counts = {}\n",
    "    for tuple in ngrams:\n",
    "        if tuple in counts:\n",
    "            counts[tuple] += 1\n",
    "        else:\n",
    "            counts[tuple] = 1\n",
    "    return counts\n",
    "\n",
    "def sort_dict_by_vals(dict):\n",
    "    sorted_dict = {\n",
    "        k: v for k, v in sorted(dict.items(), key=lambda item: item[1])\n",
    "    }\n",
    "    return sorted_dict\n",
    "\n",
    "def print_top_vals(ngram_counts, min_count, min_length):\n",
    "    \"\"\"Print ngrams with at least `min_count` counts and each component\n",
    "    exceeds `min_length` in characters.\"\"\"\n",
    "    for ngram, count in ngram_counts.items():\n",
    "        if each_elem_is_long_enough(ngram, min_length) and count >= min_count:\n",
    "            print(ngram, count)\n",
    "\n",
    "def each_elem_is_long_enough(tuple, min_length):\n",
    "    for elem in tuple:\n",
    "        if len(elem) < min_length:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "word_counts = list(ngrams(filtered_lowercase_words, 1))\n",
    "word_counts = sort_dict_by_vals(count_ngrams(word_counts))\n",
    "print_top_vals(word_counts, min_count=50, min_length=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7704d9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Unsurprisingly, salad is the most common word.\n",
    "Positive words include \"great\", \"like\", \"delicious\", \"healthy\", \"good\", and \"fresh\".\n",
    "Because the overall rating across all reviews is high, around 4.3, it's also unsurprising there are no negative words in the most frequent list.\n",
    "We can repeat the above for bigrams and trigrams, which are strings of two and three words, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef81de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bigrams = list(ngrams(filtered_lowercase_words, 2))\n",
    "bigram_counts = sort_dict_by_vals(count_ngrams(bigrams))\n",
    "print_top_vals(bigram_counts, min_count=10, min_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47790e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trigrams = list(ngrams(filtered_lowercase_words, 3))\n",
    "trigram_counts = sort_dict_by_vals(count_ngrams(trigrams))\n",
    "print_top_vals(trigram_counts, min_count=5, min_length=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f366442",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "These are less informative as they are not positive word plus noun.\n",
    "Instead, they seem to be names of menu items or expected phrases, like \"Farmer's Fridge\" and \"vending machine\".\n",
    "\n",
    "Since most of the reviews are positive, it's interesting to repeat the ngram analysis for negative reviews only.\n",
    "Only about 10% of reviews have a rating of 2 or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negative_text = combine_review_text(df[df.rating <= 2])\n",
    "words = nltk.word_tokenize(negative_text)\n",
    "filtered_lowercase_words = []\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    if word not in stop_words:\n",
    "        filtered_lowercase_words.append(word)\n",
    "filtered_lowercase_words[:12]\n",
    "\n",
    "word_counts = list(ngrams(filtered_lowercase_words, 1))\n",
    "word_counts = sort_dict_by_vals(count_ngrams(word_counts))\n",
    "print_top_vals(word_counts, min_count=5, min_length=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d16ff",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "While we don't see obvious negative terms like \"bad\", we can infer from the given words that these reviewers didn't like the \"prices\", \"little\" [portions?], [available?] \"times\", etc.\n",
    "Similarly from the bigrams and trigrams,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24673630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bigrams = list(ngrams(filtered_lowercase_words, 2))\n",
    "bigram_counts = sort_dict_by_vals(count_ngrams(bigrams))\n",
    "print_top_vals(bigram_counts, min_count=2, min_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd324751",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = list(ngrams(filtered_lowercase_words, 3))\n",
    "trigram_counts = sort_dict_by_vals(count_ngrams(trigrams))\n",
    "print_top_vals(trigram_counts, min_count=2, min_length=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b6a21",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "we can infer things like these reviewers were not impressed by the salad portions or the vending machine (perhaps they were just not expecting one?).\n",
    "\n",
    "(yelp-future)=\n",
    "## Future Work\n",
    "\n",
    "These analyses would benefit from more review data and more use of data already collected in reviews.\n",
    "The Selenium web-scraping approach is very flexible and generalizable.\n",
    "Future work can automate it to search for all fridge locations in Chicago and scrape all locations with 1 or more reviews.\n",
    "It is also easy to extend this approach to other cities, like New York City.\n",
    "I collected lots of metadata, like how many other reviews a given reviewer has posted or whether they've obtained the \"Elite\" Yelp user status.\n",
    "In addition, it may be useful to take a more detailed look at reviews that have lots of engagement, measured by how many other Yelpers found the review useful, funny, and/or cool.\n",
    "\n",
    "Additional data could be estimated from the variables collected, for example reviewer sex from an analysis of their names.\n",
    "This could be useful information if Farmer's Fridge wanted to better target a particular demographic.\n",
    "More in depth natural language processing could also be useful to improve the accuracy of the popularity metrics, e.g. salad popularity.\n",
    "Rather than just looking at if \"salad\" was mentioned, analyzing the sentiment around that mention would give better information."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.7.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}